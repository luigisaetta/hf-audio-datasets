{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42b707ff-96c1-4561-b063-aa53ac6e1f60",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate Word Error Rate (WER)\n",
    "\n",
    "Imagine that you have an ASR model (for example, Whisper), and you want to evluate the performance of this model on an evaluation dataset (Librispeech?).\n",
    "* Which metric are you going to use?\n",
    "* What code can you use?\n",
    "\n",
    "In this NB we will show how you can use **Word Error Rate (WER)**, with HF **Evaluate** library\n",
    "\n",
    "More details [Here](https://huggingface.co/spaces/evaluate-metric/wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5746a6e1-4b53-40c1-9891-86d4f07d61d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# you need to have installed jiwer\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41fbe882-5e0e-4a88-b689-c20233eb0df3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wer = load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "186a7198-7f90-4f33-8ce6-65041ab64481",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"wer\", module_type: \"metric\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}, usage: \"\"\"\n",
       "Compute WER score of transcribed segments against references.\n",
       "\n",
       "Args:\n",
       "    references: List of references for each speech input.\n",
       "    predictions: List of transcriptions to score.\n",
       "    concatenate_texts (bool, default=False): Whether to concatenate all input texts or compute WER iteratively.\n",
       "\n",
       "Returns:\n",
       "    (float): the word error rate\n",
       "\n",
       "Examples:\n",
       "\n",
       "    >>> predictions = [\"this is the prediction\", \"there is an other sample\"]\n",
       "    >>> references = [\"this is the reference\", \"there is another one\"]\n",
       "    >>> wer = evaluate.load(\"wer\")\n",
       "    >>> wer_score = wer.compute(predictions=predictions, references=references)\n",
       "    >>> print(wer_score)\n",
       "    0.5\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf298115-2820-41f5-80c4-b5c97144c2bc",
   "metadata": {},
   "source": [
    "#### the perfect case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8787e80-4278-4fe6-acc4-71a534afaada",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you need to create two list of strings:\n",
    "# predictions: prediction of model\n",
    "# references: expected transcriptions\n",
    "\n",
    "preds = [\"sentence1\", \"sentence2\"]\n",
    "expected = [\"sentence1\", \"sentence2\"]\n",
    "\n",
    "wer_score = wer.compute(predictions=preds, references=expected)\n",
    "\n",
    "# between 0 and 1, the lower is better\n",
    "wer_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf00b96-c803-4bc1-8ee9-11648eac4d6f",
   "metadata": {},
   "source": [
    "#### the worst case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f2819a4-d79a-4941-b7e1-6f66e8367f54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = [\"sentence3\", \"sentence4\"]\n",
    "expected = [\"sentence1\", \"sentence2\"]\n",
    "\n",
    "wer_score = wer.compute(predictions=preds, references=expected)\n",
    "\n",
    "wer_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e79f045-b0f2-42c7-9a39-36f2eba0802b",
   "metadata": {},
   "source": [
    "As we can see, WER can be **indeed a punishing metric**. In the last example, with just two wrong chars we get a WER = 1 (100%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aeb1e0-18d1-4b4d-b862-dbe2f1182b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
